{
  "title": "聊聊多线程程序的load balance",
  "cells": [
    {
      "type": "text",
      "data": "<div style=\"caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: &quot;Microsoft Yahei&quot;, &quot;Helvetica Neue&quot;, &quot;Luxi Sans&quot;, &quot;DejaVu Sans&quot;, Tahoma, &quot;Hiragino Sans GB&quot;, STHeiti; font-size: 16px; background-color: rgb(255, 255, 255);\"><p>说起load balance，一般比较容易想到的是大型服务在多个replica之间的load balance、和kernal的load balance。前者一般只是在流量入口做一下流量分配，逻辑相对简单；而后者则比较复杂，需要不断发现正在运行的各个进程之间的imbalance，然后通过将进程在CPU之间进行迁移，使得各个CPU都被充分利用起来。</p><p>&nbsp; &nbsp;而本文想要讨论的load balance有别于以上两种，它是多线程(多进程)server程序内部，各个worker线程(进程)之间的load balance。</p><p>&nbsp; &nbsp;考虑一种常用的server模型：一个receiver线程负责接收请求，后面有一个线程池装了一堆worker线程，收到的请求被分派给这些worker进行处理。receiver与worker之间通过pthread_cond+request_queue来进行通信。一般的做法是：receiver将收到的请求放入queue，然后signal一下cond，就OK了。具体哪个worker会被唤醒，那是kernel的事情(实际上kernel会遵循先来后到原则，唤醒先进入等待的进程，参阅《<a href=\"http://hi.baidu.com/_kouu/item/202d77dab2e866866cce3f67\" style=\"text-decoration: none;\">linux futex浅析</a>》)。通常情况下这样做就足够了，receiver唤醒worker不需要涉及load balance的逻辑。但是有时候我们还是可以做一些load balance的工作，来提高server的性能。</p><h2>kernel load balance概述</h2><p>&nbsp; &nbsp;由于这里的load balance跟kernel的load balance息息相关，所以我们有必要先看看kernel的load balance都做了些什么。详细的内容请参阅《<a href=\"http://hi.baidu.com/_kouu/item/479891211a84e3c9a5275ad9\" style=\"text-decoration: none;\">linux内核SMP负载均衡浅析</a>》，这里只做一些简要的概括。</p><p>&nbsp; &nbsp;说白了，kernel的load balance就做一件事情：&nbsp;<strong>让系统中RUNNING状态的进程尽可能的被分摊，在每一个调度域上看都是balance的</strong>&nbsp;。怎么理解呢？现在CPU的结构一般有：物理CPU、core、超线程、这么几个层次。”在每一个调度域上看都balance”可以理解为在每一个层次上都balance：每个物理CPU上的总load相当、每个core上的总load相当、每个超线程上的load也相当。</p><p>&nbsp; &nbsp;我们在系统中看到的”CPU”都是最底层的超线程这个层次，我们可能会直观的认为把RUNNING状态的进程分摊到每一个”CPU”上就行了，但是实际上kernel的load balance还有更高的要求。假设我们的机器有2个物理CPU、每个物理CPU有2个core、每个core有2个超线程，共8个”CPU”。如果现在有8个RUNNING状态的进程(假设优先级都相同)，每个”CPU”各分摊一个进程，那么自然就是balance的。但是如果现在只有4个RUNNING状态的进程(假设优先级都相同)，真正的balance并不仅仅是每个进程各自落到一个”CPU”上就行了，而是进一步要求每个物理CPU上跑两个进程、每个core上跑一个进程。</p><p>&nbsp; &nbsp;为什么要有这样的强约束呢？因为尽管各个”CPU”逻辑上是独立的(不存在主从关系之类)，但它们并非孤立存在。相同物理CPU下的”CPU”会共享cache、相同core下的”CPU”会共享计算资源(所谓的超线程也就是一套流水线跑两个线程)。而共享也就意味着争抢。所以，在RUNNING状态的进程并非正好均摊给每一个”CPU”的情况下，需要考虑更高层次的CPU是否被均摊，以避免cache和CPU流水线的争抢(当然，除了性能，这也体现了kernel的公平性)。</p><p>&nbsp; &nbsp;最后再多提一点，kernel的load balance是异步的。为避免占用过多资源，kernel肯定不可能实时监控各个”CPU”的情况，然后面对变化实时的做出反应(当然，实时进程除外，但这不在我们讨论范围内)。</p><h2>server的load balance考虑</h2><p>&nbsp; &nbsp;有了kernel的load balance作为铺垫，看看我们server上的receiver线程能做些什么吧。</p><p>&nbsp; &nbsp;首先是worker线程的数量问题。如果worker数量过多会发生什么情况？还是假设我们的机器有上述的8个”CPU”，假设我们开了80个worker，再假设这80个线程被平均分派到每一个”CPU”上，等待处理任务。当一堆请求陆续到来的时候，由于我们的receiver没有任何load balance的策略，被唤醒的worker出现在哪个”CPU”上可以说是随机的。你想想，”同时”到来的8个请求正好落到8个不同”CPU”上的概率是多少？是：(70*60*50*40*30*20*10)/(79*78*77*76*75*74*73)=0.34%。也就是说几乎肯定会出现某些”CPU”要处理多个请求、某些”CPU”却闲着没事干的情况，系统的性能可想而知。而等到后知后觉的kernel load balance将这些请求balance到每一个”CPU”上时，可能请求已经处理得差不多了，等到下一批请求到来时，load又还是凌乱的。因为刚刚已经balance好的那些worker线程又被放回到了cond等待队列的尾部，而优先响应新请求的则是那些位于队列头部的未曾被balance过的worker。</p><p>&nbsp; &nbsp;那么会不会经历几轮请求之后就能达到balance了呢？如果请求真的是一轮一轮的过来，并且每个请求的处理时间完全相同，那么有可能会达到balance，但是实际情况肯定相差甚远。</p><p>&nbsp; &nbsp;解决办法是什么呢？将cond先进先出的队列式等待逻辑改为后进先出的栈式逻辑，或许可以解决问题，但是更好的办法应该是限制worker的数目等于或者略小于”CPU”数目，这样很自然的就balance了。</p><p>&nbsp; &nbsp;第二个问题，既然我们承认kernel在各个调度域上的load balance的有意义的，我们server中的receiver线程是不是也可以通过类似的办法来获得收益呢？现在我们吸取了之前的教训，只开了8个worker线程。依靠kernel load balance的作用，这8个线程基本会固定在每一个”CPU”上。假设现在一下子来了4个请求，它们会落到4个不同的”CPU”上，如果运气好，这4个”CPU”分别属于不同的core，那么处理请求的过程就不会涉及CPU资源的争抢；反之，可能形成2个core非常忙、2个core闲着的局面。</p><p>&nbsp; &nbsp;要解决这个问题需要做到两点，继续以我们之前的server程序为例。首先，receiver线程要知道各个worker线程都落在哪一个”CPU”上；然后在分派任务时还需要有balance的眼光。要做到第一点，最好是借助sched_affinity功能将线程固定在某个”CPU”上，避免kernel load balance把问题搞复杂了。既然前面我们已经得出了工作线程数等于或略小于CPU数的结论，现在每个线程固定在一个CPU上就是可行的。第二点，我们需要在现有pthread_cond的基础上做一些改进，给进入等待状态的worker线程赋一个优先级，比如每个core的第一个超线程作为第一优先级，第二个超线程为第二优先级。那么在cond唤醒工作线程的时候，我们就可以尽量让worker线程不落到同一个core上。实现上可以利用futex的bitset系列功能，通过bitset来标识优先级，以便在唤醒指定的worker线程。(参阅《<a href=\"http://hi.baidu.com/_kouu/item/202d77dab2e866866cce3f67\" style=\"text-decoration: none;\">linux futex浅析</a>》。)</p><h2>例子</h2><p>&nbsp; &nbsp;好了，纸上谈兵讲了这么多，得来点实际的例子验证一下。为了简单，就不写什么server程序了，只需要一个生产者线程和若干消费者线程。生产者线程生成一些任务，通过cond+queue将其传递给消费者线程。为了观察在不同任务负载下的程序表现，我们需要控制任务负载。消费者线程在完成任务后通过另一组cond+queue把任务应答给生产者线程，于是生产者就知道当前有多少个任务正在处理中，以便控制生产新任务的节奏。最后，我们通过观察在不同条件下完成一批任务的时间来体会程序的性能。</p><p>&nbsp; &nbsp;这里面比较关键的是任务本身的处理逻辑，既然我们讨论的是CPU的负载，任务肯定应该是CPU密集型的任务。然后，单个任务的处理时间不宜太短，否则可能调度过程会成为程序的瓶颈，体现不出CPU的负载问题；另一方面，单个任务的处理时间也不宜太长，否则后知后觉的kernel load balance也能解决问题，体现不出我们主动做load balance的好处(比如任务处理时间是10秒，kernel load balance花费几十毫秒来解决balance问题其实也无伤大雅)。</p><p>&nbsp; &nbsp;代码贴在文章最后，编译出来的bin文件是这样的：</p><pre>$g++ cond.cpp -pthread -O2\n$./a.out\nusage: ./a.out -j job_kind=shm|calc [-t thread_count=1] \n[-o job_load=1] [-c job_count=10] [-a affinity=0] [-l] \n[-f filename=\"./TEST\" -n filelength=128M]\n</pre><ul><li><p>代码里面准备了两种任务逻辑，”-j shm”是mmap一个文件，然后读取上面的数据做一些运算(文件及其长度由-f和-n参数来限定)；”-j calc”是做一些算术运算；</p></li><li><p>“-t”参数指定工作线程的线程数；</p></li><li><p>“-o”指定任务负载；</p></li><li><p>“-c”指定单个线程处理任务的个数；</p></li><li><p>“-a”指定是否设置sched_affinity，并且指明跳几个”CPU”放一个worker线程。比如”-a 1″表示把worker线程顺序固定在1、2、3、……号”CPU”上，而”-a 2″表示固定在2、4、6、……号”CPU”上，以此类推。需要注意的是，邻近的”CPU”号并不表示”CPU”在物理上是邻近的，比如在我测试用的机器上，共24个”CPU”，0~11号是每个core的第一个超线程、12~23是第二个超线程。这个细节需要读/proc/cpuinfo来确定。</p></li><li><p>“-l”参数指定启用我们增强版的分级cond，启用的话会将0~11号worker作为第一优先级，12~23作为第二优先级(当然，需要配合”-a”参数才有实际意义，否则也不确定这些worker都落在哪些”CPU”上)；</p></li><p>&nbsp; &nbsp;首先来看worker线程过多所带来的问题(以下case各运行5次取时间最小值)。</p><pre>case-1，启240个worker线程，24个任务负载：\n$./a.out -j calc -t 240 -o 24\ntotal cost: 23790\n$./a.out -j shm -t 240 -o 24\ntotal cost: 16827\n\ncase-2，启24个worker线程，24个任务负载：\n$./a.out -j calc -t 24 -o 24\ntotal cost: 23210\n$./a.out -j shm -t 24 -o 24\ntotal cost: 16121\n</pre><p>&nbsp; &nbsp;case-2效果明显要好略一些。并且在运行过程中如果用top观察的话，你会发现case-1只能压到2200%左右的CPU，而case-2几乎能达到2400%。</p><p>&nbsp; &nbsp;在case-1的基础上，如果禁止kernel load balance会怎样？加affinity试试看：</p><pre>case-3，启240个worker线程，24个任务负载，加affinity：\n$./a.out -j calc -t 240 -o 24 -a 1\ntotal cost: 27170\n$./a.out -j shm -t 240 -o 24 -a 1\ntotal cost: 15351\n</pre><p>&nbsp; &nbsp;calc任务比较符合预期，没有kernel load balance的情况下，性能继续下降。</p><p>&nbsp; &nbsp;而shm任务则让人大跌眼镜，性能居然提升了！其实这个任务除了CPU之外还很依赖于内存，因为所有任务都工作在同一个文件的mmap上，”CPU”挨得近反而更能发挥内存cache。(可见在这种情况下，kernel load balance其实是帮了倒忙。)</p><p>&nbsp; &nbsp;那么，我们将工作线程再调回24，是不是应该更理想？</p><pre>case-3'\n$./a.out -j shm -t 24 -o 24 -a 1\ntotal cost: 15133\n</pre><p>&nbsp; &nbsp;再来看第二个问题，worker线程站位不均所带来的影响。</p><pre>case-4，启24个worker线程，12个任务负载：\n$./a.out -j calc -t 24 -o 12\ntotal cost: 14686\n$./a.out -j shm -t 24 -o 12\ntotal cost: 13265\n\ncase-5，启24个worker线程，12个任务负载，加affinity，启用分级cond：\n$./a.out -j calc -t 24 -o 12 -a 1 -l\ntotal cost: 12206\n$./a.out -j shm -t 24 -o 12 -a 1 -l\ntotal cost: 12376\n</pre><p>&nbsp; &nbsp;效果还是不错的。改一下”-a”参数，让同一个core的两个超线程都分在同一优先级呢？</p><pre>case-5'\n$./a.out -j calc -t 24 -o 12 -a 2 -l\ntotal cost: 23510\n$./a.out -j shm -t 24 -o 12 -a 2 -l\ntotal cost: 15063\n</pre><p>&nbsp; &nbsp;由于争抢CPU资源，calc任务性能变得很差，几乎减半。而shm任务由于cache复用所带来的好处，情况还好(比case-3还略好一些)。</p><p>&nbsp; &nbsp;这里的任务只是举了calc和shm两个例子，实际情况可能是很复杂的。尽管load balance的问题肯定存在，但是任务会因共享cache而得利、还是因争抢cache而失利？争抢CPU流水线又会造成多大的损失？这些都只能具体问题具体分析。kernel的load balance将负载尽量均摊到离得远的”CPU”上，大多数情况下没有问题。不过我们也看到shm任务中cache共享的收益还是很大的，如果例子更极端一点，肯定会出现承受负载的CPU离得越近，反而效果越好的情况。</p><p>&nbsp; &nbsp;另一方面，争抢CPU流水线会有多大损失，也可以简单的分析一下。超线程相当于两个线程共用一套CPU流水线，如果单个线程的代码上下文依赖很严重，指令基本上只能串行工作，无法充分利用流水线，那么流水线的空余能力就可以留给第二个线程使用。反之如果一个线程就能把流水线填满，硬塞两个线程进来肯定就只能有50%的性能(上述calc的例子就差不多是这样)。</p><p>&nbsp; &nbsp;为了说明这个问题，我们给calc任务加了一个SERIAL_CALC的宏开关，让它的运算逻辑变成上下文强依赖。然后重跑case-5中的两个命令，我们会看到其实在这种情况下承受负载的CPU离得近一些似乎也问题不大：</p><pre>case-6，采用SERIAL_CALC运算逻辑，重跑case-5中的calc任务\n$g++ cond.cpp -pthread -O2 -DSERIAL_CALC\n$./a.out -j calc -t 24 -o 12 -a 1 -l\ntotal cost: 51269\n$./a.out -j calc -t 24 -o 12 -a 2 -l\ntotal cost: 56753\n</pre><p>&nbsp; &nbsp;最后是代码，有兴趣你还可以尝试更多的case，have fun！</p><pre>#include &lt;pthread.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;sched.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;string.h&gt;\n#include &lt;linux/futex.h&gt;\n#include &lt;sys/time.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;sys/mman.h&gt;\n#include &lt;math.h&gt;\n#include &lt;sys/syscall.h&gt;\n\n#define CPUS    24\n#define FUTEX_WAIT_BITSET   9\n#define FUTEX_WAKE_BITSET   10\n\nstruct Job\n{\n    long _input;\n    long _output;\n};\n\nclass JobRunner\n{\npublic:\n    virtual void run(Job* job) = 0;\n};\n\nclass ShmJobRunner : public JobRunner\n{\npublic:\n    ShmJobRunner(const char* filepath, size_t length)\n            : _length(length) {\n        int fd = open(filepath, O_RDONLY);\n        _base = (long*)mmap(NULL, _length*sizeof(long),\n                PROT_READ, MAP_SHARED|MAP_POPULATE, fd, 0);\n        if (_base == MAP_FAILED) {\n            printf(\"FATAL: mmap %s(%lu) failed!\\n\",\n                    filepath, _length*sizeof(long));\n            abort();\n        }\n        close(fd);\n    }\n    virtual void run(Job* job) {\n        long i = job-&gt;_input % _length;\n        long j = i + _length - 1;\n        const int step = 4;\n        while (i + step &lt; j) {\n            if (_base[i%_length] * _base[j%_length] &gt; 0) {\n                j -= step;\n            }\n            else {\n                i += step;\n            }\n        }\n        job-&gt;_output = _base[i%_length];\n    }\nprivate:\n    const long* _base;\n    size_t _length;\n};\n\nclass CalcJobRunner : public JobRunner\n{\npublic:\n    virtual void run(Job* job) {\n        long v1 = 1;\n        long v2 = 1;\n        long v3 = 1;\n        for (int i = 0; i &lt; job-&gt;_input; i++) {\n#ifndef SERIAL_CALC\n            v1 += v2 + v3;\n            v2 *= 3;\n            v3 *= 5;\n#else\n            v1 += v2 + v3;\n            v2 = v1 * 5 + v2 * v3;\n            v3 = v1 * 3 + v1 * v2;\n#endif\n        }\n        job-&gt;_output = v1;\n    }\n};\n\nclass JobRunnerCreator\n{\npublic:\n    static JobRunner* create(const char* name,\n            const char* filepath, size_t filelength) {\n        if (strcmp(name, \"shm\") == 0) {\n            printf(\"share memory job\\n\");\n            return new ShmJobRunner(filepath, filelength);\n        }\n        else if (strcmp(name, \"calc\") == 0) {\n            printf(\"caculation job\\n\");\n            return new CalcJobRunner();\n        }\n        printf(\"unknown job '%s'\\n\", name);\n        return NULL;\n    }\n};\n\nclass Cond\n{\npublic:\n    virtual void lock() = 0;\n    virtual void unlock() = 0;\n    virtual void wait(size_t) = 0;\n    virtual void wake() = 0;\n};\n\nclass NormalCond : public Cond\n{\npublic:\n    NormalCond() {\n        pthread_mutex_init(&amp;_mutex, NULL);\n        pthread_cond_init(&amp;_cond, NULL);\n    }\n    ~NormalCond() {\n        pthread_mutex_destroy(&amp;_mutex);\n        pthread_cond_destroy(&amp;_cond);\n    }\n    void lock() { pthread_mutex_lock(&amp;_mutex); }\n    void unlock() { pthread_mutex_unlock(&amp;_mutex); }\n    void wait(size_t) { pthread_cond_wait(&amp;_cond, &amp;_mutex); }\n    void wake() { pthread_cond_signal(&amp;_cond); }\nprivate:\n    pthread_mutex_t _mutex;\n    pthread_cond_t _cond;\n};\n\nclass LayeredCond : public Cond\n{\npublic:\n    LayeredCond(size_t layers = 1) : _value(0), _layers(layers) {\n        pthread_mutex_init(&amp;_mutex, NULL);\n        if (_layers &gt; sizeof(int)*8) {\n            printf(\"FATAL: cannot support such layer %u (max %u)\\n\",\n                    _layers, sizeof(int)*8);\n            abort();\n        }\n        _waiters = new size_t[_layers];\n        memset(_waiters, 0, sizeof(size_t)*_layers);\n    }\n    ~LayeredCond() {\n        pthread_mutex_destroy(&amp;_mutex);\n        delete _waiters;\n        _waiters = NULL;\n    }\n    void lock() {\n        pthread_mutex_lock(&amp;_mutex);\n    }\n    void unlock() {\n        pthread_mutex_unlock(&amp;_mutex);\n    }\n    void wait(size_t layer) {\n        if (layer &gt;= _layers) {\n            printf(\"FATAL: layer overflow (%u/%u)\\n\", layer, _layers);\n            abort();\n        }\n        _waiters[layer]++;\n        while (_value == 0) {\n            int value = _value;\n            unlock();\n            syscall(__NR_futex, &amp;_value, FUTEX_WAIT_BITSET, value,\n                    NULL, NULL, layer2mask(layer));\n            lock();\n        }\n        _waiters[layer]--;\n        _value--;\n    }\n    void wake() {\n        int mask = ~0;\n        lock();\n        for (size_t i = 0; i &lt; _layers; i++) {\n            if (_waiters[i] &gt; 0) {\n                mask = layer2mask(i);\n                break;\n            }\n        }\n        _value++;\n        unlock();\n        syscall(__NR_futex, &amp;_value, FUTEX_WAKE_BITSET, 1,\n                NULL, NULL, mask);\n    }\nprivate:\n    int layer2mask(size_t layer) {\n        return 1 &lt;&lt; layer;\n    }\nprivate:\n    pthread_mutex_t _mutex;\n    int _value;\n    size_t* _waiters;\n    size_t _layers;\n};\n\ntemplate&lt;class T&gt;\nclass Stack\n{\npublic:\n    Stack(size_t size, size_t cond_layers = 0) : _size(size), _sp(0) {\n        _buf = new T*[_size];\n        _cond = (cond_layers &gt; 0) ?\n            (Cond*)new LayeredCond(cond_layers) : (Cond*)new NormalCond();\n    }\n    ~Stack() {\n        delete []_buf;\n        delete _cond;\n    }\n    T* pop(size_t layer = 0) {\n        T* ret = NULL;\n        _cond-&gt;lock();\n        do {\n            if (_sp &gt; 0) {\n                ret = _buf[--_sp];\n            }\n            else {\n                _cond-&gt;wait(layer);\n            }\n        } while (ret == NULL);\n        _cond-&gt;unlock();\n        return ret;\n    }\n    void push(T* obj) {\n        _cond-&gt;lock();\n        if (_sp &gt;= _size) {\n            printf(\"FATAL: stack overflow\\n\");\n            abort();\n        }\n        _buf[_sp++] = obj;\n        _cond-&gt;unlock();\n        _cond-&gt;wake();\n    }\nprivate:\n    const size_t _size;\n    size_t _sp;\n    T** _buf;\n    Cond* _cond;\n};\n\ninline struct timeval cost_begin()\n{\n    struct timeval tv;\n    gettimeofday(&amp;tv, NULL);\n    return tv;\n}\n\ninline long cost_end(struct timeval &amp;tv)\n{\n    struct timeval tv2;\n    gettimeofday(&amp;tv2, NULL);\n    tv2.tv_sec -= tv.tv_sec;\n    tv2.tv_usec -= tv.tv_usec;\n    return tv2.tv_sec*1000+tv2.tv_usec/1000;\n}\n\nstruct ThreadParam\n{\n    size_t layer;\n    Stack&lt;Job&gt;* inputQ;\n    Stack&lt;Job&gt;* outputQ;\n    JobRunner* runner;\n};\n\nvoid* thread_func(void *data)\n{\n    size_t layer = ((ThreadParam*)data)-&gt;layer;\n    Stack&lt;Job&gt;* inputQ = ((ThreadParam*)data)-&gt;inputQ;\n    Stack&lt;Job&gt;* outputQ = ((ThreadParam*)data)-&gt;outputQ;\n    JobRunner* runner = ((ThreadParam*)data)-&gt;runner;\n\n    while (1) {\n        Job* job = inputQ-&gt;pop(layer);\n        runner-&gt;run(job);\n        outputQ-&gt;push(job);\n    }\n    return NULL;\n}\n\nvoid force_cpu(pthread_t t, int n)\n{\n    cpu_set_t cpus;\n    CPU_ZERO(&amp;cpus);\n    CPU_SET(n, &amp;cpus);\n    if (pthread_setaffinity_np(t, sizeof(cpus), &amp;cpus) != 0) {\n        printf(\"FATAL: force cpu %d failed: %s\\n\", n, strerror(errno));\n        abort();\n    }\n}\n\nvoid usage(const char* bin)\n{\n    printf(\"usage: %s -j job_kind=shm|calc \"\n        \"[-t thread_count=1] [-o job_load=1] [-c job_count=10] \"\n        \"[-a affinity=0] [-l] \"\n        \"[-f filename=\"./TEST\" -n filelength=128M]\\n\", bin);\n    abort();\n}\n\nint main(int argc, char* const* argv)\n{\n    int THREAD_COUNT = 1;\n    int JOB_LOAD = 1;\n    int JOB_COUNT = 10;\n    int AFFINITY = 0;\n    int LAYER = 0;\n    char JOB_KIND[16] = \"\";\n    char FILEPATH[1024] = \"./TEST\";\n    size_t LENGTH = 128*1024*1024;\n    for (int i = EOF;\n        (i = getopt(argc, argv, \"t:o:c:a:j:lf:n:\")) != EOF;) {\n        switch (i) {\n        case 't': THREAD_COUNT = atoi(optarg); break;\n        case 'o': JOB_LOAD = atoi(optarg); break;\n        case 'c': JOB_COUNT = atoi(optarg); break;\n        case 'a': AFFINITY = atoi(optarg); break;\n        case 'l': LAYER = 2; break;\n        case 'j': strncpy(JOB_KIND, optarg, sizeof(JOB_KIND)-1); break;\n        case 'f': strncpy(FILEPATH, optarg, sizeof(FILEPATH)-1); break;\n        case 'n': LENGTH = atoi(optarg); break;\n        default: usage(argv[0]); break;\n        }\n    }\n    JobRunner* runner = JobRunnerCreator::create(\n            JOB_KIND, FILEPATH, LENGTH);\n    if (!runner) {\n        usage(argv[0]);\n    }\n\n    srand(0);\n    Job jobs[JOB_LOAD];\n\n#ifdef TEST_LOAD\n    for (int i = 0; i &lt; JOB_LOAD; i++) {\n        jobs[i]._input = rand();\n        struct timeval tv = cost_begin();\n        runner-&gt;run(&amp;jobs[i]);\n        long cost = cost_end(tv);\n        printf(\"job[%d](%ld)=(%ld) costs: %ld\\n\",\n                i, jobs[i]._input, jobs[i]._output, cost);\n    }\n    delete runner;\n    return 0;\n#endif\n\n    printf(\"use layer %d\\n\", LAYER);\n    Stack&lt;Job&gt; inputQ(JOB_LOAD, LAYER);\n    Stack&lt;Job&gt; outputQ(JOB_LOAD, LAYER);\n\n    pthread_t t;\n    ThreadParam param[THREAD_COUNT];\n\n    printf(\"thread init: \");\n    for (int i = 0; i &lt; THREAD_COUNT; i++) {\n        int cpu = AFFINITY ? (i/AFFINITY+i%AFFINITY*CPUS/2)%CPUS : -1;\n        size_t layer = !!(LAYER &amp;&amp; i % CPUS &gt;= CPUS/2);\n        param[i].inputQ = &amp;inputQ;\n        param[i].outputQ = &amp;outputQ;\n        param[i].runner = runner;\n        param[i].layer = layer;\n        pthread_create(&amp;t, NULL, thread_func, (void*)&amp;param[i]);\n        if (cpu &gt;= 0) {\n            printf(\"%d(%d|%d),\", i, cpu, layer);\n            force_cpu(t, cpu);\n        }\n        else {\n            printf(\"%d(*|%d),\", i, layer);\n        }\n        usleep(1000);\n    }\n    printf(\"\\n\");\n\n    struct timeval tv = cost_begin();\n    for (int i = 0; i &lt; JOB_LOAD; i++) {\n        jobs[i]._input = rand();\n        inputQ.push(&amp;jobs[i]);\n    }\n    for (int i = 0; i &lt; JOB_LOAD*JOB_COUNT; i++) {\n        Job* job = outputQ.pop();\n        job-&gt;_input = rand();\n        inputQ.push(job);\n    }\n    for (int i = 0; i &lt; JOB_LOAD; i++) {\n        outputQ.pop();\n    }\n    long cost = cost_end(tv);\n    printf(\"total cost: %ld\\n\", cost);\n\n    delete runner;\n    return 0;\n}\n</pre><li></li></ul></div><div style=\"caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0); font-family: &quot;Microsoft Yahei&quot;, &quot;Helvetica Neue&quot;, &quot;Luxi Sans&quot;, &quot;DejaVu Sans&quot;, Tahoma, &quot;Hiragino Sans GB&quot;, STHeiti; font-size: 16px; background-color: rgb(255, 255, 255);\"><a href=\"http://www.atatech.org/articles/24451/follow\" rel=\"nofollow\" style=\"text-decoration: none;\"></a></div>"
    }
  ]
}